{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur_9grkLN3lx"
      },
      "outputs": [],
      "source": [
        "#Cleaning Splitting and Saving Data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "\n",
        "# Ensure the output folder exists\n",
        "output_dir = \"processed_data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load and clean\n",
        "df = pd.read_csv(\"creditcard.csv\").dropna()\n",
        "\n",
        "# Features and target\n",
        "features = ['Time'] + [f'V{i}' for i in range(1, 29)] + ['Amount']\n",
        "X = df[features]\n",
        "y = df['Class']\n",
        "\n",
        "# Split (save once)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save processed split to disk\n",
        "pd.DataFrame(X_train_scaled, columns=features).to_csv(os.path.join(output_dir, \"X_train_scaled.csv\"), index=False)\n",
        "pd.DataFrame(X_test_scaled, columns=features).to_csv(os.path.join(output_dir, \"X_test_scaled.csv\"), index=False)\n",
        "y_train.to_csv(os.path.join(output_dir, \"y_train.csv\"), index=False)\n",
        "y_test.to_csv(os.path.join(output_dir, \"y_test.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Evaluate Logistic Regression\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Define balancing techniques\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train, y_train),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    # Train Logistic Regression\n",
        "    model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "    model.fit(X_res, y_res)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "results_df.to_excel(\"results/logistic_regression_results.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "ohkY7jbEOEJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Evaluate XGBoost\n",
        "\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Define balancing techniques\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train, y_train),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    # Train XGBoost\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbosity=0)\n",
        "    model.fit(X_res, y_res)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "excel_path = \"results/xgboost_results.xlsx\"\n",
        "results_df.to_excel(excel_path, index=False)"
      ],
      "metadata": {
        "id": "YYerjFlAONE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Evaluate K-Nearest Neighbours\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Define balancing techniques\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train, y_train),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "# Fast grid search parameters for KNN\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5],\n",
        "    'weights': ['uniform'],\n",
        "    'metric': ['euclidean']\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    # Downsample for speed (only use 10k rows max)\n",
        "    if len(X_res) > 10000:\n",
        "        X_res = X_res.sample(n=10000, random_state=42)\n",
        "        y_res = y_res.loc[X_res.index]\n",
        "\n",
        "    # Grid Search with 2-fold CV\n",
        "    grid = GridSearchCV(KNeighborsClassifier(), param_grid,\n",
        "                        cv=2, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_res, y_res)\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    # Predict\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Best Params\": str(grid.best_params_),\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "excel_path = \"results/knn_results.xlsx\"\n",
        "results_df.to_excel(excel_path, index=False)"
      ],
      "metadata": {
        "id": "UCuAnpW2OYmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Evaluate Linear Discriminant Analysis\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Define balancing techniques\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train, y_train),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "# Grid search parameters for LDA (limited, because LDA has few tunable params)\n",
        "param_grid = {\n",
        "    'solver': ['svd', 'lsqr', 'eigen']\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    # Grid Search with 2-fold CV using full data\n",
        "    grid = GridSearchCV(LinearDiscriminantAnalysis(), param_grid,\n",
        "                        cv=2, scoring='f1', n_jobs=-1, error_score='raise')\n",
        "    grid.fit(X_res, y_res)\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    # Predict\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Best Params\": str(grid.best_params_),\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "excel_path = \"results/lda_results.xlsx\"\n",
        "results_df.to_excel(excel_path, index=False)"
      ],
      "metadata": {
        "id": "8_SzME_JOhFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Evaluate Gaussian Naive Bayes\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Define balancing techniques\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train, y_train),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    model = GaussianNB()\n",
        "    model.fit(X_res, y_res)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Best Params\": \"-\",  # no hyperparameters to tune\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "excel_path = \"results/gaussian_nb_results.xlsx\"\n",
        "results_df.to_excel(excel_path, index=False)"
      ],
      "metadata": {
        "id": "VSxsGg2LOon-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Evaluate AdaBoost\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Define balancing techniques\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train, y_train),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "# Grid search parameters for AdaBoost (small grid for speed)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    # Grid Search with 2-fold CV using full data\n",
        "    grid = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid,\n",
        "                        cv=2, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_res, y_res)\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    # Predict\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Best Params\": str(grid.best_params_),\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "excel_path = \"results/adaboost_results.xlsx\"\n",
        "results_df.to_excel(excel_path, index=False)\n",
        "\n",
        "# Optional: Auto-download in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(excel_path)\n",
        "except:\n",
        "    print(f\"üìÅ File saved locally: {excel_path}\")\n",
        "\n",
        "print(\"‚úÖ AdaBoost evaluation complete. Results saved to Excel.\")"
      ],
      "metadata": {
        "id": "xuMxUGlAOy-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate Decision Tree\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Define balancing techniques\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train, y_train),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "# Minimal grid to keep it fast but effective\n",
        "param_grid = {\n",
        "    'max_depth': [5, None],\n",
        "    'min_samples_split': [2],\n",
        "    'min_samples_leaf': [1],\n",
        "    'criterion': ['gini']\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    # Grid Search with 2-fold CV for speed\n",
        "    grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid,\n",
        "                        cv=2, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_res, y_res)\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    # Predict\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Best Params\": str(grid.best_params_),\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "excel_path = \"results/decision_tree_results.xlsx\"\n",
        "results_df.to_excel(excel_path, index=False)"
      ],
      "metadata": {
        "id": "NsczJmt6PCyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate Random Forest\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Define balancing techniques\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train, y_train),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train, y_train),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "# Minimal grid for fast Random Forest tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50],\n",
        "    'max_depth': [10, None],\n",
        "    'min_samples_split': [2],\n",
        "    'min_samples_leaf': [1],\n",
        "    'criterion': ['gini']\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    # Grid Search with 2-fold CV for speed\n",
        "    grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid,\n",
        "                        cv=2, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_res, y_res)\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    # Predict\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Best Params\": str(grid.best_params_),\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "excel_path = \"results/random_forest_results.xlsx\"\n",
        "results_df.to_excel(excel_path, index=False)"
      ],
      "metadata": {
        "id": "xzftAekfPFen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and Evaluate SVM\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "X_train = pd.read_csv(\"processed_data/X_train_scaled.csv\")\n",
        "X_test = pd.read_csv(\"processed_data/X_test_scaled.csv\")\n",
        "y_train = pd.read_csv(\"processed_data/y_train.csv\").squeeze()\n",
        "y_test = pd.read_csv(\"processed_data/y_test.csv\").squeeze()\n",
        "\n",
        "# Limit training size to 30000 samples for faster execution\n",
        "X_train_small = X_train.sample(n=30000, random_state=42)\n",
        "y_train_small = y_train.loc[X_train_small.index]\n",
        "\n",
        "# Define balancing techniques on subset\n",
        "balancing_methods = {\n",
        "    \"Unbalanced\": (X_train_small, y_train_small),\n",
        "    \"RandomOversampling\": RandomOverSampler(random_state=42).fit_resample(X_train_small, y_train_small),\n",
        "    \"RandomUndersampling\": RandomUnderSampler(random_state=42).fit_resample(X_train_small, y_train_small),\n",
        "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train_small, y_train_small)\n",
        "}\n",
        "\n",
        "# Minimal grid for fast SVM tuning\n",
        "param_grid = {\n",
        "    'C': [1],\n",
        "    'kernel': ['linear'],\n",
        "    'probability': [True]\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for method_name, (X_res, y_res) in balancing_methods.items():\n",
        "    # Grid Search with 2-fold CV for speed\n",
        "    grid = GridSearchCV(SVC(), param_grid, cv=2, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_res, y_res)\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    # Predict\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate\n",
        "    results.append({\n",
        "        \"Balancing Method\": method_name,\n",
        "        \"Best Params\": str(grid.best_params_),\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
        "        \"Confusion Matrix\": str(confusion_matrix(y_test, y_pred).tolist())\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "excel_path = \"results/svm_results_subset.xlsx\"\n",
        "results_df.to_excel(excel_path, index=False)"
      ],
      "metadata": {
        "id": "rUXM_EkcPPEV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}